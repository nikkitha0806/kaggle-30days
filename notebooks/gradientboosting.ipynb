{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\nvenkatesan\\anaconda3\\lib\\site-packages (1.7.5)\n",
      "Requirement already satisfied: scipy in c:\\users\\nvenkatesan\\anaconda3\\lib\\site-packages (from xgboost) (1.9.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\nvenkatesan\\anaconda3\\lib\\site-packages (from xgboost) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries:\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data:\n",
    "data = pd.read_csv(\"../data/Melbourne_housing_FULL.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 27247 entries, 1 to 34856\n",
      "Data columns (total 21 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Suburb         27247 non-null  object \n",
      " 1   Address        27247 non-null  object \n",
      " 2   Rooms          27247 non-null  int64  \n",
      " 3   Type           27247 non-null  object \n",
      " 4   Price          27247 non-null  float64\n",
      " 5   Method         27247 non-null  object \n",
      " 6   SellerG        27247 non-null  object \n",
      " 7   Date           27247 non-null  object \n",
      " 8   Distance       27246 non-null  float64\n",
      " 9   Postcode       27246 non-null  float64\n",
      " 10  Bedroom2       20806 non-null  float64\n",
      " 11  Bathroom       20800 non-null  float64\n",
      " 12  Car            20423 non-null  float64\n",
      " 13  Landsize       17982 non-null  float64\n",
      " 14  BuildingArea   10656 non-null  float64\n",
      " 15  YearBuilt      12084 non-null  float64\n",
      " 16  CouncilArea    27244 non-null  object \n",
      " 17  Lattitude      20993 non-null  float64\n",
      " 18  Longtitude     20993 non-null  float64\n",
      " 19  Regionname     27244 non-null  object \n",
      " 20  Propertycount  27244 non-null  float64\n",
      "dtypes: float64(12), int64(1), object(8)\n",
      "memory usage: 4.6+ MB\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#drop all the null vlaues based on the Price column:\n",
    "data.dropna(subset=[\"Price\"],inplace=True)\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using certain columns:\n",
    "cols_to_use = [\"Rooms\", \"Distance\", \"Landsize\", \"BuildingArea\", \"YearBuilt\"]\n",
    "X = data[cols_to_use]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select target:\n",
    "y = data.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seperate data into training and validation sets:\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be exploring XGBoost library. XGboost stands for extreme gradient boosting which is an implementation of gradient boosting with several additional features focused on performance and speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBRegressor()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class XGBRegressor in module xgboost.sklearn:\n",
      "\n",
      "class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      " |  XGBRegressor(*, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
      " |  \n",
      " |  Implementation of the scikit-learn API for XGBoost regression.\n",
      " |  \n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  \n",
      " |      n_estimators : int\n",
      " |          Number of gradient boosted trees.  Equivalent to number of boosting\n",
      " |          rounds.\n",
      " |  \n",
      " |      max_depth :  Optional[int]\n",
      " |          Maximum tree depth for base learners.\n",
      " |      max_leaves :\n",
      " |          Maximum number of leaves; 0 indicates no limit.\n",
      " |      max_bin :\n",
      " |          If using histogram-based algorithm, maximum number of bins per feature\n",
      " |      grow_policy :\n",
      " |          Tree growing policy. 0: favor splitting at nodes closest to the node, i.e. grow\n",
      " |          depth-wise. 1: favor splitting at nodes with highest loss change.\n",
      " |      learning_rate : Optional[float]\n",
      " |          Boosting learning rate (xgb's \"eta\")\n",
      " |      verbosity : Optional[int]\n",
      " |          The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      " |      objective : typing.Union[str, typing.Callable[[numpy.ndarray, numpy.ndarray], typing.Tuple[numpy.ndarray, numpy.ndarray]], NoneType]\n",
      " |          Specify the learning task and the corresponding learning objective or\n",
      " |          a custom objective function to be used (see note below).\n",
      " |      booster: Optional[str]\n",
      " |          Specify which booster to use: gbtree, gblinear or dart.\n",
      " |      tree_method: Optional[str]\n",
      " |          Specify which tree method to use.  Default to auto.  If this parameter is set to\n",
      " |          default, XGBoost will choose the most conservative option available.  It's\n",
      " |          recommended to study this option from the parameters document :doc:`tree method\n",
      " |          </treemethod>`\n",
      " |      n_jobs : Optional[int]\n",
      " |          Number of parallel threads used to run xgboost.  When used with other\n",
      " |          Scikit-Learn algorithms like grid search, you may choose which algorithm to\n",
      " |          parallelize and balance the threads.  Creating thread contention will\n",
      " |          significantly slow down both algorithms.\n",
      " |      gamma : Optional[float]\n",
      " |          (min_split_loss) Minimum loss reduction required to make a further partition on a\n",
      " |          leaf node of the tree.\n",
      " |      min_child_weight : Optional[float]\n",
      " |          Minimum sum of instance weight(hessian) needed in a child.\n",
      " |      max_delta_step : Optional[float]\n",
      " |          Maximum delta step we allow each tree's weight estimation to be.\n",
      " |      subsample : Optional[float]\n",
      " |          Subsample ratio of the training instance.\n",
      " |      sampling_method :\n",
      " |          Sampling method. Used only by `gpu_hist` tree method.\n",
      " |            - `uniform`: select random training instances uniformly.\n",
      " |            - `gradient_based` select random training instances with higher probability when\n",
      " |              the gradient and hessian are larger. (cf. CatBoost)\n",
      " |      colsample_bytree : Optional[float]\n",
      " |          Subsample ratio of columns when constructing each tree.\n",
      " |      colsample_bylevel : Optional[float]\n",
      " |          Subsample ratio of columns for each level.\n",
      " |      colsample_bynode : Optional[float]\n",
      " |          Subsample ratio of columns for each split.\n",
      " |      reg_alpha : Optional[float]\n",
      " |          L1 regularization term on weights (xgb's alpha).\n",
      " |      reg_lambda : Optional[float]\n",
      " |          L2 regularization term on weights (xgb's lambda).\n",
      " |      scale_pos_weight : Optional[float]\n",
      " |          Balancing of positive and negative weights.\n",
      " |      base_score : Optional[float]\n",
      " |          The initial prediction score of all instances, global bias.\n",
      " |      random_state : Optional[Union[numpy.random.RandomState, int]]\n",
      " |          Random number seed.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             Using gblinear booster with shotgun updater is nondeterministic as\n",
      " |             it uses Hogwild algorithm.\n",
      " |  \n",
      " |      missing : float, default np.nan\n",
      " |          Value in the data which needs to be present as a missing value.\n",
      " |      num_parallel_tree: Optional[int]\n",
      " |          Used for boosting random forest.\n",
      " |      monotone_constraints : Optional[Union[Dict[str, int], str]]\n",
      " |          Constraint of variable monotonicity.  See :doc:`tutorial </tutorials/monotonic>`\n",
      " |          for more information.\n",
      " |      interaction_constraints : Optional[Union[str, List[Tuple[str]]]]\n",
      " |          Constraints for interaction representing permitted interactions.  The\n",
      " |          constraints must be specified in the form of a nested list, e.g. ``[[0, 1], [2,\n",
      " |          3, 4]]``, where each inner list is a group of indices of features that are\n",
      " |          allowed to interact with each other.  See :doc:`tutorial\n",
      " |          </tutorials/feature_interaction_constraint>` for more information\n",
      " |      importance_type: Optional[str]\n",
      " |          The feature importance type for the feature_importances\\_ property:\n",
      " |  \n",
      " |          * For tree model, it's either \"gain\", \"weight\", \"cover\", \"total_gain\" or\n",
      " |            \"total_cover\".\n",
      " |          * For linear model, only \"weight\" is defined and it's the normalized coefficients\n",
      " |            without bias.\n",
      " |  \n",
      " |      gpu_id : Optional[int]\n",
      " |          Device ordinal.\n",
      " |      validate_parameters : Optional[bool]\n",
      " |          Give warnings for unknown parameter.\n",
      " |      predictor : Optional[str]\n",
      " |          Force XGBoost to use specific predictor, available choices are [cpu_predictor,\n",
      " |          gpu_predictor].\n",
      " |      enable_categorical : bool\n",
      " |  \n",
      " |          .. versionadded:: 1.5.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Experimental support for categorical data.  When enabled, cudf/pandas.DataFrame\n",
      " |          should be used to specify categorical data type.  Also, JSON/UBJSON\n",
      " |          serialization format is required.\n",
      " |  \n",
      " |      feature_types : FeatureTypes\n",
      " |  \n",
      " |          .. versionadded:: 1.7.0\n",
      " |  \n",
      " |          Used for specifying feature types without constructing a dataframe. See\n",
      " |          :py:class:`DMatrix` for details.\n",
      " |  \n",
      " |      max_cat_to_onehot : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          A threshold for deciding whether XGBoost should use one-hot encoding based split\n",
      " |          for categorical data.  When number of categories is lesser than the threshold\n",
      " |          then one-hot encoding is chosen, otherwise the categories will be partitioned\n",
      " |          into children nodes. Also, `enable_categorical` needs to be set to have\n",
      " |          categorical feature support. See :doc:`Categorical Data\n",
      " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      " |  \n",
      " |      max_cat_threshold : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.7.0\n",
      " |  \n",
      " |          .. note:: This parameter is experimental\n",
      " |  \n",
      " |          Maximum number of categories considered for each split. Used only by\n",
      " |          partition-based splits for preventing over-fitting. Also, `enable_categorical`\n",
      " |          needs to be set to have categorical feature support. See :doc:`Categorical Data\n",
      " |          </tutorials/categorical>` and :ref:`cat-param` for details.\n",
      " |  \n",
      " |      eval_metric : Optional[Union[str, List[str], Callable]]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Metric used for monitoring the training result and early stopping.  It can be a\n",
      " |          string or list of strings as names of predefined metric in XGBoost (See\n",
      " |          doc/parameter.rst), one of the metrics in :py:mod:`sklearn.metrics`, or any other\n",
      " |          user defined metric that looks like `sklearn.metrics`.\n",
      " |  \n",
      " |          If custom objective is also provided, then custom metric should implement the\n",
      " |          corresponding reverse link function.\n",
      " |  \n",
      " |          Unlike the `scoring` parameter commonly used in scikit-learn, when a callable\n",
      " |          object is provided, it's assumed to be a cost function and by default XGBoost will\n",
      " |          minimize the result during early stopping.\n",
      " |  \n",
      " |          For advanced usage on Early stopping like directly choosing to maximize instead of\n",
      " |          minimize, see :py:obj:`xgboost.callback.EarlyStopping`.\n",
      " |  \n",
      " |          See :doc:`Custom Objective and Evaluation Metric </tutorials/custom_metric_obj>`\n",
      " |          for more.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |               This parameter replaces `eval_metric` in :py:meth:`fit` method.  The old one\n",
      " |               receives un-transformed prediction regardless of whether custom objective is\n",
      " |               being used.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              from sklearn.datasets import load_diabetes\n",
      " |              from sklearn.metrics import mean_absolute_error\n",
      " |              X, y = load_diabetes(return_X_y=True)\n",
      " |              reg = xgb.XGBRegressor(\n",
      " |                  tree_method=\"hist\",\n",
      " |                  eval_metric=mean_absolute_error,\n",
      " |              )\n",
      " |              reg.fit(X, y, eval_set=[(X, y)])\n",
      " |  \n",
      " |      early_stopping_rounds : Optional[int]\n",
      " |  \n",
      " |          .. versionadded:: 1.6.0\n",
      " |  \n",
      " |          Activates early stopping. Validation metric needs to improve at least once in\n",
      " |          every **early_stopping_rounds** round(s) to continue training.  Requires at least\n",
      " |          one item in **eval_set** in :py:meth:`fit`.\n",
      " |  \n",
      " |          The method returns the model from the last iteration (not the best one).  If\n",
      " |          there's more than one item in **eval_set**, the last entry will be used for early\n",
      " |          stopping.  If there's more than one metric in **eval_metric**, the last metric\n",
      " |          will be used for early stopping.\n",
      " |  \n",
      " |          If early stopping occurs, the model will have three additional fields:\n",
      " |          :py:attr:`best_score`, :py:attr:`best_iteration` and\n",
      " |          :py:attr:`best_ntree_limit`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |              This parameter replaces `early_stopping_rounds` in :py:meth:`fit` method.\n",
      " |  \n",
      " |      callbacks : Optional[List[TrainingCallback]]\n",
      " |          List of callback functions that are applied at end of each iteration.\n",
      " |          It is possible to use predefined callbacks by using\n",
      " |          :ref:`Callback API <callback_api>`.\n",
      " |  \n",
      " |          .. note::\n",
      " |  \n",
      " |             States in callback are not preserved during training, which means callback\n",
      " |             objects can not be reused for multiple training sessions without\n",
      " |             reinitialization or deepcopy.\n",
      " |  \n",
      " |          .. code-block:: python\n",
      " |  \n",
      " |              for params in parameters_grid:\n",
      " |                  # be sure to (re)initialize the callbacks before each run\n",
      " |                  callbacks = [xgb.callback.LearningRateScheduler(custom_rates)]\n",
      " |                  xgboost.train(params, Xy, callbacks=callbacks)\n",
      " |  \n",
      " |      kwargs : dict, optional\n",
      " |          Keyword arguments for XGBoost Booster object.  Full documentation of parameters\n",
      " |          can be found :doc:`here </parameter>`.\n",
      " |          Attempting to set a parameter via the constructor args and \\*\\*kwargs\n",
      " |          dict simultaneously will result in a TypeError.\n",
      " |  \n",
      " |          .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      " |  \n",
      " |              \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee\n",
      " |              that parameters passed via this argument will interact properly\n",
      " |              with scikit-learn.\n",
      " |  \n",
      " |          .. note::  Custom objective function\n",
      " |  \n",
      " |              A custom objective function can be provided for the ``objective``\n",
      " |              parameter. In this case, it should have the signature\n",
      " |              ``objective(y_true, y_pred) -> grad, hess``:\n",
      " |  \n",
      " |              y_true: array_like of shape [n_samples]\n",
      " |                  The target values\n",
      " |              y_pred: array_like of shape [n_samples]\n",
      " |                  The predicted values\n",
      " |  \n",
      " |              grad: array_like of shape [n_samples]\n",
      " |                  The value of the gradient for each sample point.\n",
      " |              hess: array_like of shape [n_samples]\n",
      " |                  The value of the second derivative for each sample point\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      XGBRegressor\n",
      " |      XGBModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, objective: Union[str, Callable[[numpy.ndarray, numpy.ndarray], Tuple[numpy.ndarray, numpy.ndarray]], NoneType] = 'reg:squarederror', **kwargs: Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from XGBModel:\n",
      " |  \n",
      " |  __sklearn_is_fitted__(self) -> bool\n",
      " |  \n",
      " |  apply(self, X: Any, ntree_limit: int = 0, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Return the predicted leaf every tree for each sample. If the model is trained with\n",
      " |      early stopping, then `best_iteration` is used automatically.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like, shape=[n_samples, n_features]\n",
      " |          Input features matrix.\n",
      " |      \n",
      " |      iteration_range :\n",
      " |          See :py:meth:`predict`.\n",
      " |      \n",
      " |      ntree_limit :\n",
      " |          Deprecated, use ``iteration_range`` instead.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      " |          For each datapoint x in X and for each tree, return the index of the\n",
      " |          leaf x ends up in. Leaves are numbered within\n",
      " |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      " |  \n",
      " |  evals_result(self) -> Dict[str, Dict[str, List[float]]]\n",
      " |      Return the evaluation results.\n",
      " |      \n",
      " |      If **eval_set** is passed to the :py:meth:`fit` function, you can call\n",
      " |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.  When\n",
      " |      **eval_metric** is also passed to the :py:meth:`fit` function, the\n",
      " |      **evals_result** will contain the **eval_metrics** passed to the :py:meth:`fit`\n",
      " |      function.\n",
      " |      \n",
      " |      The returned evaluation result is a dictionary:\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      " |           'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      evals_result\n",
      " |  \n",
      " |  fit(self, X: Any, y: Any, *, sample_weight: Optional[Any] = None, base_margin: Optional[Any] = None, eval_set: Optional[Sequence[Tuple[Any, Any]]] = None, eval_metric: Union[str, Sequence[str], Callable[[numpy.ndarray, xgboost.core.DMatrix], Tuple[str, float]], NoneType] = None, early_stopping_rounds: Optional[int] = None, verbose: Union[bool, int, NoneType] = True, xgb_model: Union[xgboost.core.Booster, ForwardRef('XGBModel'), str, NoneType] = None, sample_weight_eval_set: Optional[Sequence[Any]] = None, base_margin_eval_set: Optional[Sequence[Any]] = None, feature_weights: Optional[Any] = None, callbacks: Optional[Sequence[xgboost.callback.TrainingCallback]] = None) -> 'XGBModel'\n",
      " |      Fit gradient boosting model.\n",
      " |      \n",
      " |      Note that calling ``fit()`` multiple times will cause the model object to be\n",
      " |      re-fit from scratch. To resume training from a previous checkpoint, explicitly\n",
      " |      pass ``xgb_model`` argument.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Feature matrix\n",
      " |      y :\n",
      " |          Labels\n",
      " |      sample_weight :\n",
      " |          instance weights\n",
      " |      base_margin :\n",
      " |          global bias for each instance.\n",
      " |      eval_set :\n",
      " |          A list of (X, y) tuple pairs to use as validation sets, for which\n",
      " |          metrics will be computed.\n",
      " |          Validation metrics will help us track the performance of the model.\n",
      " |      \n",
      " |      eval_metric : str, list of str, or callable, optional\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `eval_metric` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |      \n",
      " |      early_stopping_rounds : int\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `early_stopping_rounds` in :py:meth:`__init__` or\n",
      " |              :py:meth:`set_params` instead.\n",
      " |      verbose :\n",
      " |          If `verbose` is True and an evaluation set is used, the evaluation metric\n",
      " |          measured on the validation set is printed to stdout at each boosting stage.\n",
      " |          If `verbose` is an integer, the evaluation metric is printed at each `verbose`\n",
      " |          boosting stage. The last boosting stage / the boosting stage found by using\n",
      " |          `early_stopping_rounds` is also printed.\n",
      " |      xgb_model :\n",
      " |          file name of stored XGBoost model or 'Booster' instance XGBoost model to be\n",
      " |          loaded before training (allows training continuation).\n",
      " |      sample_weight_eval_set :\n",
      " |          A list of the form [L_1, L_2, ..., L_n], where each L_i is an array like\n",
      " |          object storing instance weights for the i-th validation set.\n",
      " |      base_margin_eval_set :\n",
      " |          A list of the form [M_1, M_2, ..., M_n], where each M_i is an array like\n",
      " |          object storing base margin for the i-th validation set.\n",
      " |      feature_weights :\n",
      " |          Weight for each feature, defines the probability of each feature being\n",
      " |          selected when colsample is being used.  All values must be greater than 0,\n",
      " |          otherwise a `ValueError` is thrown.\n",
      " |      \n",
      " |      callbacks :\n",
      " |          .. deprecated:: 1.6.0\n",
      " |              Use `callbacks` in :py:meth:`__init__` or :py:meth:`set_params` instead.\n",
      " |  \n",
      " |  get_booster(self) -> xgboost.core.Booster\n",
      " |      Get the underlying xgboost Booster of this model.\n",
      " |      \n",
      " |      This will raise an exception when fit was not called\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      booster : a xgboost booster of underlying model\n",
      " |  \n",
      " |  get_num_boosting_rounds(self) -> int\n",
      " |      Gets the number of xgboost boosting rounds.\n",
      " |  \n",
      " |  get_params(self, deep: bool = True) -> Dict[str, Any]\n",
      " |      Get parameters.\n",
      " |  \n",
      " |  get_xgb_params(self) -> Dict[str, Any]\n",
      " |      Get xgboost specific parameters.\n",
      " |  \n",
      " |  load_model(self, fname: Union[str, bytearray, os.PathLike]) -> None\n",
      " |      Load the model from a file or bytearray. Path to file can be local\n",
      " |      or as an URI.\n",
      " |      \n",
      " |      The model is loaded from XGBoost format which is universal among the various\n",
      " |      XGBoost interfaces. Auxiliary attributes of the Python Booster object (such as\n",
      " |      feature_names) will not be loaded when using binary format.  To save those\n",
      " |      attributes, use JSON/UBJ instead.  See :doc:`Model IO </tutorials/saving_model>`\n",
      " |      for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.load_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.load_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname :\n",
      " |          Input file name or memory buffer(see also save_raw)\n",
      " |  \n",
      " |  predict(self, X: Any, output_margin: bool = False, ntree_limit: Optional[int] = None, validate_features: bool = True, base_margin: Optional[Any] = None, iteration_range: Optional[Tuple[int, int]] = None) -> numpy.ndarray\n",
      " |      Predict with `X`.  If the model is trained with early stopping, then `best_iteration`\n",
      " |      is used automatically.  For tree models, when data is on GPU, like cupy array or\n",
      " |      cuDF dataframe and `predictor` is not specified, the prediction is run on GPU\n",
      " |      automatically, otherwise it will run on CPU.\n",
      " |      \n",
      " |      .. note:: This function is only thread safe for `gbtree` and `dart`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X :\n",
      " |          Data to predict with.\n",
      " |      output_margin :\n",
      " |          Whether to output the raw untransformed margin value.\n",
      " |      ntree_limit :\n",
      " |          Deprecated, use `iteration_range` instead.\n",
      " |      validate_features :\n",
      " |          When this is True, validate that the Booster's and data's feature_names are\n",
      " |          identical.  Otherwise, it is assumed that the feature_names are the same.\n",
      " |      base_margin :\n",
      " |          Margin added to prediction.\n",
      " |      iteration_range :\n",
      " |          Specifies which layer of trees are used in prediction.  For example, if a\n",
      " |          random forest is trained with 100 rounds.  Specifying ``iteration_range=(10,\n",
      " |          20)``, then only the forests built during [10, 20) (half open set) rounds\n",
      " |          are used in this prediction.\n",
      " |      \n",
      " |          .. versionadded:: 1.4.0\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction\n",
      " |  \n",
      " |  save_model(self, fname: Union[str, os.PathLike]) -> None\n",
      " |      Save the model to a file.\n",
      " |      \n",
      " |      The model is saved in an XGBoost internal format which is universal among the\n",
      " |      various XGBoost interfaces. Auxiliary attributes of the Python Booster object\n",
      " |      (such as feature_names) will not be saved when using binary format.  To save\n",
      " |      those attributes, use JSON/UBJ instead. See :doc:`Model IO\n",
      " |      </tutorials/saving_model>` for more info.\n",
      " |      \n",
      " |      .. code-block:: python\n",
      " |      \n",
      " |        model.save_model(\"model.json\")\n",
      " |        # or\n",
      " |        model.save_model(\"model.ubj\")\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or os.PathLike\n",
      " |          Output file name\n",
      " |  \n",
      " |  set_params(self, **params: Any) -> 'XGBModel'\n",
      " |      Set the parameters of this estimator.  Modification of the sklearn method to\n",
      " |      allow unknown kwargs. This allows using the full range of xgboost\n",
      " |      parameters that are not defined as member variables in sklearn grid\n",
      " |      search.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from XGBModel:\n",
      " |  \n",
      " |  best_iteration\n",
      " |      The best iteration obtained by early stopping.  This attribute is 0-based,\n",
      " |      for instance if the best iteration is the first round, then best_iteration is 0.\n",
      " |  \n",
      " |  best_ntree_limit\n",
      " |  \n",
      " |  best_score\n",
      " |      The best score obtained by early stopping.\n",
      " |  \n",
      " |  coef_\n",
      " |      Coefficients property\n",
      " |      \n",
      " |      .. note:: Coefficients are defined only for linear learners\n",
      " |      \n",
      " |          Coefficients are only defined when the linear model is chosen as\n",
      " |          base learner (`booster=gblinear`). It is not defined for other base\n",
      " |          learner types, such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      Feature importances property, return depends on `importance_type`\n",
      " |      parameter. When model trained with multi-class/multi-label/multi-target dataset,\n",
      " |      the feature importance is \"averaged\" over all targets. The \"average\" is defined\n",
      " |      based on the importance type. For instance, if the importance type is\n",
      " |      \"total_gain\", then the score is sum of loss change for each split from all\n",
      " |      trees.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : array of shape ``[n_features]`` except for multi-class\n",
      " |      linear model, which returns an array with shape `(n_features, n_classes)`\n",
      " |  \n",
      " |  feature_names_in_\n",
      " |      Names of features seen during :py:meth:`fit`.  Defined only when `X` has feature\n",
      " |      names that are all strings.\n",
      " |  \n",
      " |  intercept_\n",
      " |      Intercept (bias) property\n",
      " |      \n",
      " |      .. note:: Intercept is defined only for linear learners\n",
      " |      \n",
      " |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      " |          learner (`booster=gblinear`). It is not defined for other base learner types,\n",
      " |          such as tree learners (`booster=gbtree`).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      " |  \n",
      " |  n_features_in_\n",
      " |      Number of features seen during :py:meth:`fit`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(XGBRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute error: 233543.7948186564\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute error:\",mean_absolute_error(y_true= y_valid, y_pred= predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using hyper parameter tuning concept:\n",
    "model = XGBRegressor(n_estimators = 500)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:930233.96475\n",
      "[1]\tvalidation_0-rmse:729387.88152\n",
      "[2]\tvalidation_0-rmse:601873.96149\n",
      "[3]\tvalidation_0-rmse:524734.76561\n",
      "[4]\tvalidation_0-rmse:480463.24049\n",
      "[5]\tvalidation_0-rmse:455242.98085\n",
      "[6]\tvalidation_0-rmse:440291.23348\n",
      "[7]\tvalidation_0-rmse:431241.10005\n",
      "[8]\tvalidation_0-rmse:425534.53403\n",
      "[9]\tvalidation_0-rmse:420260.47598\n",
      "[10]\tvalidation_0-rmse:415404.51754\n",
      "[11]\tvalidation_0-rmse:412354.36725\n",
      "[12]\tvalidation_0-rmse:411347.01764\n",
      "[13]\tvalidation_0-rmse:407462.40742\n",
      "[14]\tvalidation_0-rmse:404887.64221\n",
      "[15]\tvalidation_0-rmse:404360.31942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nvenkatesan\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16]\tvalidation_0-rmse:403613.93647\n",
      "[17]\tvalidation_0-rmse:403261.80371\n",
      "[18]\tvalidation_0-rmse:402084.47104\n",
      "[19]\tvalidation_0-rmse:400969.56995\n",
      "[20]\tvalidation_0-rmse:398978.17846\n",
      "[21]\tvalidation_0-rmse:398299.32400\n",
      "[22]\tvalidation_0-rmse:398386.68008\n",
      "[23]\tvalidation_0-rmse:397391.58782\n",
      "[24]\tvalidation_0-rmse:397665.11123\n",
      "[25]\tvalidation_0-rmse:397683.45255\n",
      "[26]\tvalidation_0-rmse:397675.85862\n",
      "[27]\tvalidation_0-rmse:396279.39797\n",
      "[28]\tvalidation_0-rmse:396204.57983\n",
      "[29]\tvalidation_0-rmse:396327.72699\n",
      "[30]\tvalidation_0-rmse:396051.49242\n",
      "[31]\tvalidation_0-rmse:396164.07517\n",
      "[32]\tvalidation_0-rmse:395867.91283\n",
      "[33]\tvalidation_0-rmse:395585.64529\n",
      "[34]\tvalidation_0-rmse:395133.81752\n",
      "[35]\tvalidation_0-rmse:394682.58238\n",
      "[36]\tvalidation_0-rmse:394703.85290\n",
      "[37]\tvalidation_0-rmse:394513.19498\n",
      "[38]\tvalidation_0-rmse:393071.91983\n",
      "[39]\tvalidation_0-rmse:393464.62724\n",
      "[40]\tvalidation_0-rmse:392466.29536\n",
      "[41]\tvalidation_0-rmse:392424.28729\n",
      "[42]\tvalidation_0-rmse:392547.24056\n",
      "[43]\tvalidation_0-rmse:392040.96779\n",
      "[44]\tvalidation_0-rmse:392041.27589\n",
      "[45]\tvalidation_0-rmse:391845.67806\n",
      "[46]\tvalidation_0-rmse:391095.32657\n",
      "[47]\tvalidation_0-rmse:390633.06144\n",
      "[48]\tvalidation_0-rmse:390600.94656\n",
      "[49]\tvalidation_0-rmse:390643.13292\n",
      "[50]\tvalidation_0-rmse:390455.27306\n",
      "[51]\tvalidation_0-rmse:390420.98795\n",
      "[52]\tvalidation_0-rmse:390203.03972\n",
      "[53]\tvalidation_0-rmse:390027.95185\n",
      "[54]\tvalidation_0-rmse:389368.24197\n",
      "[55]\tvalidation_0-rmse:389533.00813\n",
      "[56]\tvalidation_0-rmse:389126.55431\n",
      "[57]\tvalidation_0-rmse:388881.30372\n",
      "[58]\tvalidation_0-rmse:388567.26302\n",
      "[59]\tvalidation_0-rmse:388259.66028\n",
      "[60]\tvalidation_0-rmse:388238.41210\n",
      "[61]\tvalidation_0-rmse:388268.99379\n",
      "[62]\tvalidation_0-rmse:388238.40788\n",
      "[63]\tvalidation_0-rmse:388238.72228\n",
      "[64]\tvalidation_0-rmse:387850.98301\n",
      "[65]\tvalidation_0-rmse:387774.35980\n",
      "[66]\tvalidation_0-rmse:387630.45700\n",
      "[67]\tvalidation_0-rmse:387695.77061\n",
      "[68]\tvalidation_0-rmse:387662.45201\n",
      "[69]\tvalidation_0-rmse:387577.10585\n",
      "[70]\tvalidation_0-rmse:387576.68088\n",
      "[71]\tvalidation_0-rmse:387537.21299\n",
      "[72]\tvalidation_0-rmse:387366.13166\n",
      "[73]\tvalidation_0-rmse:387486.33059\n",
      "[74]\tvalidation_0-rmse:387246.42611\n",
      "[75]\tvalidation_0-rmse:387191.92421\n",
      "[76]\tvalidation_0-rmse:387267.72735\n",
      "[77]\tvalidation_0-rmse:387480.37787\n",
      "[78]\tvalidation_0-rmse:387701.85582\n",
      "[79]\tvalidation_0-rmse:387540.73332\n",
      "[80]\tvalidation_0-rmse:387651.63867\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=500, n_jobs=None, num_parallel_tree=None,\n",
       "             predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using early stopping : it indicates the number of iterations as where to stop\n",
    "model = XGBRegressor(n_estimators = 500)\n",
    "model.fit(X_train, y_train,\n",
    "          early_stopping_rounds = 5,\n",
    "          eval_set = [(X_valid, y_valid)],\n",
    "          verbose = True\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:1186456.99992\n",
      "[1]\tvalidation_0-rmse:1136861.12036\n",
      "[2]\tvalidation_0-rmse:1089996.84471\n",
      "[3]\tvalidation_0-rmse:1045925.06906\n",
      "[4]\tvalidation_0-rmse:1004498.13112\n",
      "[5]\tvalidation_0-rmse:965098.46242\n",
      "[6]\tvalidation_0-rmse:927785.01898\n",
      "[7]\tvalidation_0-rmse:892977.76100\n",
      "[8]\tvalidation_0-rmse:860183.32457\n",
      "[9]\tvalidation_0-rmse:829510.44454\n",
      "[10]\tvalidation_0-rmse:800864.91401\n",
      "[11]\tvalidation_0-rmse:773704.81771\n",
      "[12]\tvalidation_0-rmse:748066.91890\n",
      "[13]\tvalidation_0-rmse:724255.25526\n",
      "[14]\tvalidation_0-rmse:701955.19119\n",
      "[15]\tvalidation_0-rmse:681341.78306\n",
      "[16]\tvalidation_0-rmse:662043.39758\n",
      "[17]\tvalidation_0-rmse:643721.87088\n",
      "[18]\tvalidation_0-rmse:626496.34006\n",
      "[19]\tvalidation_0-rmse:610978.38965\n",
      "[20]\tvalidation_0-rmse:596589.80819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nvenkatesan\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:835: UserWarning: `early_stopping_rounds` in `fit` method is deprecated for better compatibility with scikit-learn, use `early_stopping_rounds` in constructor or`set_params` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\tvalidation_0-rmse:583279.29968\n",
      "[22]\tvalidation_0-rmse:570491.60726\n",
      "[23]\tvalidation_0-rmse:558740.93422\n",
      "[24]\tvalidation_0-rmse:548236.17190\n",
      "[25]\tvalidation_0-rmse:538094.37986\n",
      "[26]\tvalidation_0-rmse:528864.57893\n",
      "[27]\tvalidation_0-rmse:520111.41018\n",
      "[28]\tvalidation_0-rmse:511962.48867\n",
      "[29]\tvalidation_0-rmse:504788.95230\n",
      "[30]\tvalidation_0-rmse:498018.68851\n",
      "[31]\tvalidation_0-rmse:491895.79388\n",
      "[32]\tvalidation_0-rmse:486156.08218\n",
      "[33]\tvalidation_0-rmse:480837.01349\n",
      "[34]\tvalidation_0-rmse:475722.87882\n",
      "[35]\tvalidation_0-rmse:471255.17110\n",
      "[36]\tvalidation_0-rmse:466922.02652\n",
      "[37]\tvalidation_0-rmse:462892.97316\n",
      "[38]\tvalidation_0-rmse:459351.75532\n",
      "[39]\tvalidation_0-rmse:455804.28534\n",
      "[40]\tvalidation_0-rmse:452875.52803\n",
      "[41]\tvalidation_0-rmse:450219.21507\n",
      "[42]\tvalidation_0-rmse:447541.30922\n",
      "[43]\tvalidation_0-rmse:444913.14230\n",
      "[44]\tvalidation_0-rmse:442640.32020\n",
      "[45]\tvalidation_0-rmse:440609.39392\n",
      "[46]\tvalidation_0-rmse:438718.51532\n",
      "[47]\tvalidation_0-rmse:436771.53990\n",
      "[48]\tvalidation_0-rmse:434905.60411\n",
      "[49]\tvalidation_0-rmse:433128.40100\n",
      "[50]\tvalidation_0-rmse:431690.78649\n",
      "[51]\tvalidation_0-rmse:430051.31863\n",
      "[52]\tvalidation_0-rmse:428792.81937\n",
      "[53]\tvalidation_0-rmse:427146.93331\n",
      "[54]\tvalidation_0-rmse:425909.89474\n",
      "[55]\tvalidation_0-rmse:424835.07552\n",
      "[56]\tvalidation_0-rmse:423747.06792\n",
      "[57]\tvalidation_0-rmse:422676.06467\n",
      "[58]\tvalidation_0-rmse:421803.17758\n",
      "[59]\tvalidation_0-rmse:420860.52326\n",
      "[60]\tvalidation_0-rmse:419989.72982\n",
      "[61]\tvalidation_0-rmse:419278.16897\n",
      "[62]\tvalidation_0-rmse:418485.22835\n",
      "[63]\tvalidation_0-rmse:417464.02876\n",
      "[64]\tvalidation_0-rmse:416622.82976\n",
      "[65]\tvalidation_0-rmse:415935.12033\n",
      "[66]\tvalidation_0-rmse:415228.89902\n",
      "[67]\tvalidation_0-rmse:414537.08985\n",
      "[68]\tvalidation_0-rmse:413816.33125\n",
      "[69]\tvalidation_0-rmse:413279.91269\n",
      "[70]\tvalidation_0-rmse:412724.57476\n",
      "[71]\tvalidation_0-rmse:411951.77526\n",
      "[72]\tvalidation_0-rmse:411481.27425\n",
      "[73]\tvalidation_0-rmse:410945.23854\n",
      "[74]\tvalidation_0-rmse:410116.04804\n",
      "[75]\tvalidation_0-rmse:409625.85710\n",
      "[76]\tvalidation_0-rmse:409141.76835\n",
      "[77]\tvalidation_0-rmse:408438.11923\n",
      "[78]\tvalidation_0-rmse:408039.10421\n",
      "[79]\tvalidation_0-rmse:407779.92947\n",
      "[80]\tvalidation_0-rmse:407288.24674\n",
      "[81]\tvalidation_0-rmse:406876.07446\n",
      "[82]\tvalidation_0-rmse:406298.55327\n",
      "[83]\tvalidation_0-rmse:405864.88926\n",
      "[84]\tvalidation_0-rmse:405568.09559\n",
      "[85]\tvalidation_0-rmse:405359.67374\n",
      "[86]\tvalidation_0-rmse:404918.58271\n",
      "[87]\tvalidation_0-rmse:404674.54329\n",
      "[88]\tvalidation_0-rmse:404555.54533\n",
      "[89]\tvalidation_0-rmse:404270.78642\n",
      "[90]\tvalidation_0-rmse:403858.65924\n",
      "[91]\tvalidation_0-rmse:403610.94529\n",
      "[92]\tvalidation_0-rmse:403486.69246\n",
      "[93]\tvalidation_0-rmse:403329.67069\n",
      "[94]\tvalidation_0-rmse:403195.25966\n",
      "[95]\tvalidation_0-rmse:403089.55132\n",
      "[96]\tvalidation_0-rmse:402769.21479\n",
      "[97]\tvalidation_0-rmse:402369.55463\n",
      "[98]\tvalidation_0-rmse:402090.92037\n",
      "[99]\tvalidation_0-rmse:401928.67395\n",
      "[100]\tvalidation_0-rmse:401740.62146\n",
      "[101]\tvalidation_0-rmse:401701.72675\n",
      "[102]\tvalidation_0-rmse:401630.49024\n",
      "[103]\tvalidation_0-rmse:401462.51816\n",
      "[104]\tvalidation_0-rmse:401289.35182\n",
      "[105]\tvalidation_0-rmse:401271.60364\n",
      "[106]\tvalidation_0-rmse:401003.11699\n",
      "[107]\tvalidation_0-rmse:400951.31222\n",
      "[108]\tvalidation_0-rmse:400905.02748\n",
      "[109]\tvalidation_0-rmse:400887.06735\n",
      "[110]\tvalidation_0-rmse:400760.31993\n",
      "[111]\tvalidation_0-rmse:400687.04622\n",
      "[112]\tvalidation_0-rmse:400459.33401\n",
      "[113]\tvalidation_0-rmse:400331.37289\n",
      "[114]\tvalidation_0-rmse:400356.96777\n",
      "[115]\tvalidation_0-rmse:400220.34327\n",
      "[116]\tvalidation_0-rmse:400121.70247\n",
      "[117]\tvalidation_0-rmse:400087.94811\n",
      "[118]\tvalidation_0-rmse:399863.94373\n",
      "[119]\tvalidation_0-rmse:399890.39096\n",
      "[120]\tvalidation_0-rmse:399724.67861\n",
      "[121]\tvalidation_0-rmse:399368.65318\n",
      "[122]\tvalidation_0-rmse:399276.48615\n",
      "[123]\tvalidation_0-rmse:399184.23421\n",
      "[124]\tvalidation_0-rmse:399083.17427\n",
      "[125]\tvalidation_0-rmse:399035.46652\n",
      "[126]\tvalidation_0-rmse:398929.70540\n",
      "[127]\tvalidation_0-rmse:398842.99818\n",
      "[128]\tvalidation_0-rmse:398651.38860\n",
      "[129]\tvalidation_0-rmse:398579.08414\n",
      "[130]\tvalidation_0-rmse:398453.17813\n",
      "[131]\tvalidation_0-rmse:398468.05884\n",
      "[132]\tvalidation_0-rmse:398234.96924\n",
      "[133]\tvalidation_0-rmse:398150.77369\n",
      "[134]\tvalidation_0-rmse:398107.21101\n",
      "[135]\tvalidation_0-rmse:397827.89212\n",
      "[136]\tvalidation_0-rmse:397583.58807\n",
      "[137]\tvalidation_0-rmse:397517.25988\n",
      "[138]\tvalidation_0-rmse:397337.15747\n",
      "[139]\tvalidation_0-rmse:397275.55639\n",
      "[140]\tvalidation_0-rmse:397079.42502\n",
      "[141]\tvalidation_0-rmse:397079.44764\n",
      "[142]\tvalidation_0-rmse:397010.33367\n",
      "[143]\tvalidation_0-rmse:396989.78851\n",
      "[144]\tvalidation_0-rmse:396982.54699\n",
      "[145]\tvalidation_0-rmse:396876.20269\n",
      "[146]\tvalidation_0-rmse:396595.14118\n",
      "[147]\tvalidation_0-rmse:396528.58111\n",
      "[148]\tvalidation_0-rmse:396485.43858\n",
      "[149]\tvalidation_0-rmse:396480.74777\n",
      "[150]\tvalidation_0-rmse:396465.19659\n",
      "[151]\tvalidation_0-rmse:396395.67448\n",
      "[152]\tvalidation_0-rmse:396381.06373\n",
      "[153]\tvalidation_0-rmse:396307.06751\n",
      "[154]\tvalidation_0-rmse:396165.30857\n",
      "[155]\tvalidation_0-rmse:396079.53733\n",
      "[156]\tvalidation_0-rmse:396066.22099\n",
      "[157]\tvalidation_0-rmse:396017.65830\n",
      "[158]\tvalidation_0-rmse:395939.45355\n",
      "[159]\tvalidation_0-rmse:395939.36757\n",
      "[160]\tvalidation_0-rmse:395860.44367\n",
      "[161]\tvalidation_0-rmse:395676.45832\n",
      "[162]\tvalidation_0-rmse:395656.01895\n",
      "[163]\tvalidation_0-rmse:395630.05408\n",
      "[164]\tvalidation_0-rmse:395633.17189\n",
      "[165]\tvalidation_0-rmse:395374.93549\n",
      "[166]\tvalidation_0-rmse:395104.73206\n",
      "[167]\tvalidation_0-rmse:395082.48057\n",
      "[168]\tvalidation_0-rmse:395023.88630\n",
      "[169]\tvalidation_0-rmse:394970.27655\n",
      "[170]\tvalidation_0-rmse:394985.92295\n",
      "[171]\tvalidation_0-rmse:394956.80414\n",
      "[172]\tvalidation_0-rmse:394965.65736\n",
      "[173]\tvalidation_0-rmse:394938.00767\n",
      "[174]\tvalidation_0-rmse:394920.37837\n",
      "[175]\tvalidation_0-rmse:394914.18437\n",
      "[176]\tvalidation_0-rmse:394691.20511\n",
      "[177]\tvalidation_0-rmse:394675.75179\n",
      "[178]\tvalidation_0-rmse:394464.24983\n",
      "[179]\tvalidation_0-rmse:394450.57189\n",
      "[180]\tvalidation_0-rmse:394460.78773\n",
      "[181]\tvalidation_0-rmse:394399.22416\n",
      "[182]\tvalidation_0-rmse:394402.77192\n",
      "[183]\tvalidation_0-rmse:394186.86968\n",
      "[184]\tvalidation_0-rmse:393969.45699\n",
      "[185]\tvalidation_0-rmse:393969.22771\n",
      "[186]\tvalidation_0-rmse:393876.26985\n",
      "[187]\tvalidation_0-rmse:393831.94084\n",
      "[188]\tvalidation_0-rmse:393733.40344\n",
      "[189]\tvalidation_0-rmse:393735.92220\n",
      "[190]\tvalidation_0-rmse:393725.40057\n",
      "[191]\tvalidation_0-rmse:393711.33126\n",
      "[192]\tvalidation_0-rmse:393523.15898\n",
      "[193]\tvalidation_0-rmse:393474.04512\n",
      "[194]\tvalidation_0-rmse:393458.07338\n",
      "[195]\tvalidation_0-rmse:393266.35600\n",
      "[196]\tvalidation_0-rmse:393208.88217\n",
      "[197]\tvalidation_0-rmse:392974.08619\n",
      "[198]\tvalidation_0-rmse:392912.21598\n",
      "[199]\tvalidation_0-rmse:392767.08081\n",
      "[200]\tvalidation_0-rmse:392763.86472\n",
      "[201]\tvalidation_0-rmse:392734.27023\n",
      "[202]\tvalidation_0-rmse:392692.75125\n",
      "[203]\tvalidation_0-rmse:392687.74984\n",
      "[204]\tvalidation_0-rmse:392697.70653\n",
      "[205]\tvalidation_0-rmse:392711.45370\n",
      "[206]\tvalidation_0-rmse:392549.18413\n",
      "[207]\tvalidation_0-rmse:392534.90408\n",
      "[208]\tvalidation_0-rmse:392529.92335\n",
      "[209]\tvalidation_0-rmse:392495.50028\n",
      "[210]\tvalidation_0-rmse:392397.16769\n",
      "[211]\tvalidation_0-rmse:392392.70030\n",
      "[212]\tvalidation_0-rmse:392407.41550\n",
      "[213]\tvalidation_0-rmse:392211.30868\n",
      "[214]\tvalidation_0-rmse:392133.00482\n",
      "[215]\tvalidation_0-rmse:392121.46413\n",
      "[216]\tvalidation_0-rmse:391978.60900\n",
      "[217]\tvalidation_0-rmse:391928.05901\n",
      "[218]\tvalidation_0-rmse:391876.81498\n",
      "[219]\tvalidation_0-rmse:391850.60411\n",
      "[220]\tvalidation_0-rmse:391847.07583\n",
      "[221]\tvalidation_0-rmse:391816.92546\n",
      "[222]\tvalidation_0-rmse:391761.69299\n",
      "[223]\tvalidation_0-rmse:391512.05142\n",
      "[224]\tvalidation_0-rmse:391449.51741\n",
      "[225]\tvalidation_0-rmse:391445.25462\n",
      "[226]\tvalidation_0-rmse:391450.48862\n",
      "[227]\tvalidation_0-rmse:391403.14699\n",
      "[228]\tvalidation_0-rmse:391298.17301\n",
      "[229]\tvalidation_0-rmse:391282.78684\n",
      "[230]\tvalidation_0-rmse:391259.54484\n",
      "[231]\tvalidation_0-rmse:391206.51678\n",
      "[232]\tvalidation_0-rmse:391066.97803\n",
      "[233]\tvalidation_0-rmse:390884.53159\n",
      "[234]\tvalidation_0-rmse:390682.05580\n",
      "[235]\tvalidation_0-rmse:390617.88090\n",
      "[236]\tvalidation_0-rmse:390615.08675\n",
      "[237]\tvalidation_0-rmse:390532.23499\n",
      "[238]\tvalidation_0-rmse:390485.74976\n",
      "[239]\tvalidation_0-rmse:390546.19149\n",
      "[240]\tvalidation_0-rmse:390396.18543\n",
      "[241]\tvalidation_0-rmse:390362.21695\n",
      "[242]\tvalidation_0-rmse:390360.14930\n",
      "[243]\tvalidation_0-rmse:390356.55431\n",
      "[244]\tvalidation_0-rmse:390294.66261\n",
      "[245]\tvalidation_0-rmse:390290.48929\n",
      "[246]\tvalidation_0-rmse:390225.76315\n",
      "[247]\tvalidation_0-rmse:390227.89789\n",
      "[248]\tvalidation_0-rmse:390174.16792\n",
      "[249]\tvalidation_0-rmse:390170.65372\n",
      "[250]\tvalidation_0-rmse:390155.12428\n",
      "[251]\tvalidation_0-rmse:390163.32170\n",
      "[252]\tvalidation_0-rmse:390037.07931\n",
      "[253]\tvalidation_0-rmse:390030.07884\n",
      "[254]\tvalidation_0-rmse:389877.42259\n",
      "[255]\tvalidation_0-rmse:389900.54067\n",
      "[256]\tvalidation_0-rmse:389904.19883\n",
      "[257]\tvalidation_0-rmse:389879.29150\n",
      "[258]\tvalidation_0-rmse:389851.67982\n",
      "[259]\tvalidation_0-rmse:389867.10384\n",
      "[260]\tvalidation_0-rmse:389859.15804\n",
      "[261]\tvalidation_0-rmse:389950.49869\n",
      "[262]\tvalidation_0-rmse:389902.95176\n",
      "[263]\tvalidation_0-rmse:389759.60985\n",
      "[264]\tvalidation_0-rmse:389694.19172\n",
      "[265]\tvalidation_0-rmse:389612.86778\n",
      "[266]\tvalidation_0-rmse:389635.17167\n",
      "[267]\tvalidation_0-rmse:389659.94909\n",
      "[268]\tvalidation_0-rmse:389505.78122\n",
      "[269]\tvalidation_0-rmse:389352.33723\n",
      "[270]\tvalidation_0-rmse:389259.07644\n",
      "[271]\tvalidation_0-rmse:389193.50923\n",
      "[272]\tvalidation_0-rmse:389131.14445\n",
      "[273]\tvalidation_0-rmse:389125.05429\n",
      "[274]\tvalidation_0-rmse:389125.32562\n",
      "[275]\tvalidation_0-rmse:389078.28659\n",
      "[276]\tvalidation_0-rmse:389060.57016\n",
      "[277]\tvalidation_0-rmse:389067.69980\n",
      "[278]\tvalidation_0-rmse:389083.36935\n",
      "[279]\tvalidation_0-rmse:389085.24393\n",
      "[280]\tvalidation_0-rmse:389015.23383\n",
      "[281]\tvalidation_0-rmse:389081.10895\n",
      "[282]\tvalidation_0-rmse:389048.30666\n",
      "[283]\tvalidation_0-rmse:389010.85477\n",
      "[284]\tvalidation_0-rmse:388967.77164\n",
      "[285]\tvalidation_0-rmse:389085.91143\n",
      "[286]\tvalidation_0-rmse:389078.82137\n",
      "[287]\tvalidation_0-rmse:389043.11657\n",
      "[288]\tvalidation_0-rmse:389134.75570\n",
      "[289]\tvalidation_0-rmse:389150.77191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "             colsample_bylevel=None, colsample_bynode=None,\n",
       "             colsample_bytree=None, early_stopping_rounds=None,\n",
       "             enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "             gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "             interaction_constraints=None, learning_rate=0.05, max_bin=None,\n",
       "             max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "             max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "             min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "             n_estimators=500, n_jobs=4, num_parallel_tree=None, predictor=None,\n",
       "             random_state=None, ...)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using learning rate: \n",
    "#Note: If you have lower learning rate, then you need more number of iterations to converge.\n",
    "model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n",
    "model.fit(X_train, y_train,\n",
    "          early_stopping_rounds=5,\n",
    "          eval_set=[(X_valid, y_valid)],\n",
    "          verbose=True,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note:\n",
    "    1- Having high learning rate is not good\n",
    "    2- Having low learning rate is also not good\n",
    "    3- Having very high estimators are also not good\n",
    "\n",
    "We need to find these values by building our own function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
